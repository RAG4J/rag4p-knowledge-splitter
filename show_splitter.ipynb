{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Knowledge based chunker\n",
    "In this notebook I show you an experiment to create a knowledge based chunking mechanism. The available embedders create chunks using a sentence splitter, or a max token splitter. The problem is that each chunk can contain multiple knowledge items. This can happen in one sentence, but even more in longer chunks. To implement a good RAG system, you need chunks that contain only one knowledge item. That way our query will match the best chunks. "
   ],
   "id": "17fe28356e003602"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T14:15:59.043780Z",
     "start_time": "2024-07-05T14:15:59.038101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from rag4p.indexing.input_document import InputDocument\n",
    "from rag4p.indexing.splitter import Splitter\n",
    "from rag4p.indexing.splitters.max_token_splitter import MaxTokenSplitter\n",
    "from rag4p.integrations.openai.openai_embedder import OpenAIEmbedder\n",
    "from rag4p.rag.model.chunk import Chunk\n",
    "from rag4p.rag.store.local.internal_content_store import InternalContentStore\n",
    "from rag4p.util.key_loader import KeyLoader\n",
    "\n",
    "load_dotenv()\n",
    "key_loader = KeyLoader()\n",
    "print(f\"OpenAI key is available: {key_loader.get_openai_api_key() is not None}\")"
   ],
   "id": "34876feca16f3a95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key is available: True\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Below is the input text that we will use to test the knowledge based chunker.",
   "id": "eaec731100ffcb59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T13:55:52.968458Z",
     "start_time": "2024-07-05T13:55:52.964922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"\"\"Ever thought about building your very own question-answering system? Like the one that powers Siri, Alexa, or Google Assistant? Well, we've got something awesome lined up for you! In our hands-on workshop, we'll guide you through the ins and outs of creating a question-answering system. We prefer using Python for the workshop. We have prepared a GUI that works with python. If you prefer another language, you can still do the workshop, but you will miss the GUI to test your application.\n",
    "\n",
    "You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
    " \n",
    "We’ll be teaching you how to build an entire pipeline, starting from collecting data from various sources, converting that into vectors (yeah, it’s more math, but it’s cool, we promise), and storing it so you can use it to answer all sorts of queries. It's like building your own mini Google!\n",
    "\n",
    "We've got a repository ready to help you set up everything you need on your laptop. By the end of our workshop, you'll have your question-answering system ready and running. So, why wait? Grab your laptop, bring your coding hat, and let's start building something fantastic together. Trust us, it’s going to be a blast!\n",
    "\n",
    "Some of the highlights of the workshop: \n",
    "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
    "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM, Bedrock)\n",
    "- Use a tool for content extraction (Unstructured, Llama)\n",
    "- Create your pipeline (Langchain, Custom)\n",
    "\"\"\""
   ],
   "id": "d789b36d68b4273c",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T13:53:59.688935Z",
     "start_time": "2024-07-05T13:53:59.684889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = MaxTokenSplitter(max_tokens=200)\n",
    "chunks = splitter.split(InputDocument(document_id=\"input-doc\", text=input_text, properties={}))\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk: {chunk.chunk_id} \\n {chunk.chunk_text}\")\n",
    "    print(\"----\")"
   ],
   "id": "5b84f3b39d586405",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0 \n",
      " Ever thought about building your very own question-answering system? Like the one that powers Siri, Alexa, or Google Assistant? Well, we've got something awesome lined up for you! In our hands-on workshop, we'll guide you through the ins and outs of creating a question-answering system. We prefer using Python for the workshop. We have prepared a GUI that works with python. If you prefer another language, you can still do the workshop, but you will miss the GUI to test your application.\n",
      "\n",
      "You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that\n",
      "----\n",
      "Chunk: 1 \n",
      "  my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "\n",
      "We’ll be teaching you how to build an entire pipeline, starting from collecting data from various sources, converting that into vectors (yeah, it’s more math, but it’s cool, we promise), and storing it so you can use it to answer all sorts of queries. It's like building your own mini Google!\n",
      "\n",
      "We've got a repository ready to help you set up everything you need on your laptop. By the end of our workshop, you'll have your question-answering system ready and running. So, why wait? Grab your laptop, bring your coding hat, and let's start building something fantastic together. Trust us, it’s going to be a blast!\n",
      "\n",
      "Some of the highlights of the workshop: \n",
      "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
      "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM,\n",
      "----\n",
      "Chunk: 2 \n",
      "  Bedrock)\n",
      "- Use a tool for content extraction (Unstructured, Llama)\n",
      "- Create your pipeline (Langchain, Custom)\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T13:55:40.569476Z",
     "start_time": "2024-07-05T13:55:40.565461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SectionSplitter(Splitter):\n",
    "    def split(self, input_document: InputDocument) -> List[Chunk]:\n",
    "        sections = re.split(r\"\\n\\s*\\n\", input_document.text)\n",
    "        print(f\"Num sections: {len(sections)}\")\n",
    "\n",
    "        chunks_ = []\n",
    "        for i, section in enumerate(sections):\n",
    "            chunk_ = Chunk(input_document.document_id, i, len(sections), section, input_document.properties)\n",
    "            chunks_.append(chunk_)\n",
    "\n",
    "        return chunks_\n",
    "\n",
    "    @staticmethod\n",
    "    def name() -> str:\n",
    "        return SectionSplitter.__name__"
   ],
   "id": "34669d7dbcd75ea",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T13:55:57.041053Z",
     "start_time": "2024-07-05T13:55:57.036071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = SectionSplitter()\n",
    "chunks = splitter.split(InputDocument(document_id=\"input-doc\", text=input_text, properties={}))\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk: {chunk.chunk_id}, Num chunks: {chunk.total_chunks} \\n {chunk.chunk_text}\")\n",
    "    print(\"----\")"
   ],
   "id": "94095a2216c5da51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sections: 5\n",
      "Chunk: 0, Num chunks: 5 \n",
      " Ever thought about building your very own question-answering system? Like the one that powers Siri, Alexa, or Google Assistant? Well, we've got something awesome lined up for you! In our hands-on workshop, we'll guide you through the ins and outs of creating a question-answering system. We prefer using Python for the workshop. We have prepared a GUI that works with python. If you prefer another language, you can still do the workshop, but you will miss the GUI to test your application.\n",
      "----\n",
      "Chunk: 1, Num chunks: 5 \n",
      " You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "----\n",
      "Chunk: 2, Num chunks: 5 \n",
      " We’ll be teaching you how to build an entire pipeline, starting from collecting data from various sources, converting that into vectors (yeah, it’s more math, but it’s cool, we promise), and storing it so you can use it to answer all sorts of queries. It's like building your own mini Google!\n",
      "----\n",
      "Chunk: 3, Num chunks: 5 \n",
      " We've got a repository ready to help you set up everything you need on your laptop. By the end of our workshop, you'll have your question-answering system ready and running. So, why wait? Grab your laptop, bring your coding hat, and let's start building something fantastic together. Trust us, it’s going to be a blast!\n",
      "----\n",
      "Chunk: 4, Num chunks: 5 \n",
      " Some of the highlights of the workshop: \n",
      "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
      "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM, Bedrock)\n",
      "- Use a tool for content extraction (Unstructured, Llama)\n",
      "- Create your pipeline (Langchain, Custom)\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T14:36:17.881935Z",
     "start_time": "2024-07-05T14:36:17.865625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_client = OpenAI(api_key=key_loader.get_openai_api_key())\n",
    "\n",
    "\n",
    "def fetch_knowledge_chunks(orig_chunk: Chunk) -> List[Chunk]:\n",
    "\n",
    "    prompt = f\"\"\"Task: Extract Knowledge Chunks\n",
    "    \n",
    "    Please extract knowledge chunks from the following text. Each chunk should capture distinct, self-contained units of information in a subject-description format. Return the extracted knowledge chunks as a JSON object or array, ensuring that each chunk includes both the subject and its corresponding description.\n",
    "    \n",
    "    Text:\n",
    "    {orig_chunk.chunk_text}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are an assistant that takes apart a piece of text into semantic chunks to be used in a RAG system.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False,\n",
    "    )\n",
    "    \n",
    "    answer = json.loads(completion.choices[0].message.content)\n",
    "\n",
    "    chunks_ = []\n",
    "    for index, kc in enumerate(answer[\"knowledge_chunks\"]):\n",
    "        chunk_ = Chunk(orig_chunk.get_id(), index, len(answer[\"knowledge_chunks\"]), f'{kc[\"subject\"]}: {kc[\"description\"]}', {\"original_text\": orig_chunk.chunk_text, \"original_chunk_id\": orig_chunk.get_id(), \"original_total_chunks\": orig_chunk.total_chunks})\n",
    "        chunks_.append(chunk_)\n",
    "        \n",
    "    return chunks_"
   ],
   "id": "8ba7b7bf341e94f3",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T14:39:41.493712Z",
     "start_time": "2024-07-05T14:39:36.101153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "knowledge_chunks = fetch_knowledge_chunks(chunks[1])\n",
    "for kc in knowledge_chunks:\n",
    "    print(f\"Chunk: {kc.get_id()}, Num chunks: {kc.total_chunks} \\n {kc.chunk_text} \\n Original: {kc.properties['original_text']}\")\n",
    "    print(\"----\")"
   ],
   "id": "a74d87f04bc8b723",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: input-doc_1_0, Num chunks: 5 \n",
      " Hands-on experience: You'll get your hands dirty with vector stores and Large Language Models. \n",
      " Original: You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "----\n",
      "Chunk: input-doc_1_1, Num chunks: 5 \n",
      " Combining vector stores and Large Language Models: We help you combine these two in a way you've never done before. \n",
      " Original: You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "----\n",
      "Chunk: input-doc_1_2, Num chunks: 5 \n",
      " Traditional keyword-based searches: You've probably used search engines for keyword-based searches, right? This involves entering specific keywords to get search results. \n",
      " Original: You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "----\n",
      "Chunk: input-doc_1_3, Num chunks: 5 \n",
      " Semantic search: Semantic search is the next big thing after traditional searches. It uses meanings and context rather than just keywords to find results. \n",
      " Original: You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "----\n",
      "Chunk: input-doc_1_4, Num chunks: 5 \n",
      " Example of semantic search: An example of semantic search is moving from asking Google to search 'best pizza places' to 'Where can I find a pizza place that my gluten-intolerant, vegan friend would love?' \n",
      " Original: You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from rag4p.integrations.openai import EMBEDDING_SMALL\n",
    "\n",
    "# Create an in memory content store to hold some chunks\n",
    "openai_embedder = OpenAIEmbedder(api_key=key_loader.get_openai_api_key(), embedding_model=EMBEDDING_SMALL)\n",
    "content_store = InternalContentStore(embedder=openai_embedder, metadata=None)"
   ],
   "id": "ab8f17c770adc274"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
