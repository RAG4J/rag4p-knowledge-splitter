{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Knowledge based chunker\n",
    "In this notebook I show you an experiment to create a knowledge based chunking mechanism. The available embedders create chunks using a sentence splitter, or a max token splitter. The problem is that each chunk can contain multiple knowledge items. This can happen in one sentence, but even more in longer chunks. To implement a good RAG system, you need chunks that contain only one knowledge item. That way our query will match the best chunks. \n",
    "\n",
    "This notebook makes use of the RAG4p project. "
   ],
   "id": "17fe28356e003602"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T14:22:17.926562Z",
     "start_time": "2024-07-06T14:22:17.918312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from rag4p.indexing.input_document import InputDocument\n",
    "from rag4p.indexing.splitter import Splitter\n",
    "from rag4p.indexing.splitters.max_token_splitter import MaxTokenSplitter\n",
    "from rag4p.integrations.openai.openai_answer_generator import OpenaiAnswerGenerator\n",
    "from rag4p.integrations.openai.openai_embedder import OpenAIEmbedder\n",
    "from rag4p.rag.model.chunk import Chunk\n",
    "from rag4p.rag.store.local.internal_content_store import InternalContentStore\n",
    "from rag4p.util.key_loader import KeyLoader\n",
    "\n",
    "load_dotenv()\n",
    "key_loader = KeyLoader()\n",
    "print(f\"OpenAI key is available: {key_loader.get_openai_api_key() is not None}\")"
   ],
   "id": "34876feca16f3a95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key is available: True\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Below is the input text that we will use to test the knowledge based chunker.",
   "id": "eaec731100ffcb59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T13:32:08.288039Z",
     "start_time": "2024-07-06T13:32:08.284684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"\"\"Ever thought about building your very own question-answering system? Like the one that powers Siri, Alexa, or Google Assistant? Well, we've got something awesome lined up for you! In our hands-on workshop, we'll guide you through the ins and outs of creating a question-answering system. We prefer using Python for the workshop. We have prepared a GUI that works with python. If you prefer another language, you can still do the workshop, but you will miss the GUI to test your application.\n",
    "\n",
    "You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
    " \n",
    "We’ll be teaching you how to build an entire pipeline, starting from collecting data from various sources, converting that into vectors (yeah, it’s more math, but it’s cool, we promise), and storing it so you can use it to answer all sorts of queries. It's like building your own mini Google!\n",
    "\n",
    "We've got a repository ready to help you set up everything you need on your laptop. By the end of our workshop, you'll have your question-answering system ready and running. So, why wait? Grab your laptop, bring your coding hat, and let's start building something fantastic together. Trust us, it’s going to be a blast!\n",
    "\n",
    "Some of the highlights of the workshop: \n",
    "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
    "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM, Bedrock)\n",
    "- Use a tool for content extraction (Unstructured, Llama)\n",
    "- Create your pipeline (Langchain, Custom)\n",
    "\"\"\""
   ],
   "id": "d789b36d68b4273c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T13:32:08.481139Z",
     "start_time": "2024-07-06T13:32:08.289652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = MaxTokenSplitter(max_tokens=200)\n",
    "chunks = splitter.split(InputDocument(document_id=\"input-doc\", text=input_text, properties={}))\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk: {chunk.chunk_id} \\n {chunk.chunk_text}\")\n",
    "    print(\"----\")"
   ],
   "id": "5b84f3b39d586405",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0 \n",
      " Ever thought about building your very own question-answering system? Like the one that powers Siri, Alexa, or Google Assistant? Well, we've got something awesome lined up for you! In our hands-on workshop, we'll guide you through the ins and outs of creating a question-answering system. We prefer using Python for the workshop. We have prepared a GUI that works with python. If you prefer another language, you can still do the workshop, but you will miss the GUI to test your application.\n",
      "\n",
      "You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that\n",
      "----\n",
      "Chunk: 1 \n",
      "  my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      " \n",
      "We’ll be teaching you how to build an entire pipeline, starting from collecting data from various sources, converting that into vectors (yeah, it’s more math, but it’s cool, we promise), and storing it so you can use it to answer all sorts of queries. It's like building your own mini Google!\n",
      "\n",
      "We've got a repository ready to help you set up everything you need on your laptop. By the end of our workshop, you'll have your question-answering system ready and running. So, why wait? Grab your laptop, bring your coding hat, and let's start building something fantastic together. Trust us, it’s going to be a blast!\n",
      "\n",
      "Some of the highlights of the workshop: \n",
      "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
      "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM\n",
      "----\n",
      "Chunk: 2 \n",
      " , Bedrock)\n",
      "- Use a tool for content extraction (Unstructured, Llama)\n",
      "- Create your pipeline (Langchain, Custom)\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T13:32:08.487627Z",
     "start_time": "2024-07-06T13:32:08.483955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SectionSplitter(Splitter):\n",
    "    def split(self, input_document: InputDocument) -> List[Chunk]:\n",
    "        sections = re.split(r\"\\n\\s*\\n\", input_document.text)\n",
    "        print(f\"Num sections: {len(sections)}\")\n",
    "\n",
    "        chunks_ = []\n",
    "        for i, section in enumerate(sections):\n",
    "            chunk_ = Chunk(input_document.document_id, i, len(sections), section, input_document.properties)\n",
    "            chunks_.append(chunk_)\n",
    "\n",
    "        return chunks_\n",
    "\n",
    "    @staticmethod\n",
    "    def name() -> str:\n",
    "        return SectionSplitter.__name__"
   ],
   "id": "34669d7dbcd75ea",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use the splitter to chop the input text into section chunks. The next clode block creates the chunks and prints them to show the result.",
   "id": "123a56c5fcf304eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T13:32:08.492721Z",
     "start_time": "2024-07-06T13:32:08.488962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = SectionSplitter()\n",
    "chunks = splitter.split(InputDocument(document_id=\"input-doc\", text=input_text, properties={}))\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk: {chunk.chunk_id}, Num chunks: {chunk.total_chunks} \\n {chunk.chunk_text}\")\n",
    "    print(\"----\")"
   ],
   "id": "94095a2216c5da51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sections: 5\n",
      "Chunk: 0, Num chunks: 5 \n",
      " Ever thought about building your very own question-answering system? Like the one that powers Siri, Alexa, or Google Assistant? Well, we've got something awesome lined up for you! In our hands-on workshop, we'll guide you through the ins and outs of creating a question-answering system. We prefer using Python for the workshop. We have prepared a GUI that works with python. If you prefer another language, you can still do the workshop, but you will miss the GUI to test your application.\n",
      "----\n",
      "Chunk: 1, Num chunks: 5 \n",
      " You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right?\n",
      "----\n",
      "Chunk: 2, Num chunks: 5 \n",
      " We’ll be teaching you how to build an entire pipeline, starting from collecting data from various sources, converting that into vectors (yeah, it’s more math, but it’s cool, we promise), and storing it so you can use it to answer all sorts of queries. It's like building your own mini Google!\n",
      "----\n",
      "Chunk: 3, Num chunks: 5 \n",
      " We've got a repository ready to help you set up everything you need on your laptop. By the end of our workshop, you'll have your question-answering system ready and running. So, why wait? Grab your laptop, bring your coding hat, and let's start building something fantastic together. Trust us, it’s going to be a blast!\n",
      "----\n",
      "Chunk: 4, Num chunks: 5 \n",
      " Some of the highlights of the workshop: \n",
      "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
      "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM, Bedrock)\n",
      "- Use a tool for content extraction (Unstructured, Llama)\n",
      "- Create your pipeline (Langchain, Custom)\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T07:39:00.162702Z",
     "start_time": "2024-07-07T07:39:00.141037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_client = OpenAI(api_key=key_loader.get_openai_api_key())\n",
    "\n",
    "\n",
    "def fetch_knowledge_chunks(orig_chunk: Chunk) -> List[Chunk]:\n",
    "    prompt = f\"\"\"Task: Extract Knowledge Chunks\n",
    "    \n",
    "    Please extract knowledge chunks from the following text. Each chunk should capture distinct, self-contained units of information in a subject-description format. Return the extracted knowledge chunks as a JSON object or array, ensuring that each chunk includes both the subject and its corresponding description. Use the format: {{\"knowledge_chunks\": [{{\"subject\": \"subject\", \"description\": \"description\"}}]}}\n",
    "    \n",
    "    Text:\n",
    "    {orig_chunk.chunk_text}\n",
    "    \"\"\"\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are an assistant that takes apart a piece of text into semantic chunks to be used in a RAG system.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    answer_ = json.loads(completion.choices[0].message.content)\n",
    "\n",
    "    chunks_ = []\n",
    "    if \"knowledge_chunks\" not in answer_:\n",
    "        print(f\"Error in answer: {answer_}\")\n",
    "        return chunks_\n",
    "\n",
    "    for index, know_chunk in enumerate(answer_[\"knowledge_chunks\"]):\n",
    "        chunk_ = Chunk(orig_chunk.get_id(), index, len(answer_[\"knowledge_chunks\"]),\n",
    "                       f'{know_chunk[\"subject\"]}: {know_chunk[\"description\"]}',\n",
    "                       {\"original_text\": orig_chunk.chunk_text, \"original_chunk_id\": orig_chunk.get_id(),\n",
    "                        \"original_total_chunks\": orig_chunk.total_chunks})\n",
    "        chunks_.append(chunk_)\n",
    "\n",
    "    return chunks_\n",
    "\n",
    "\n",
    "def fetch_knowledge_question_chunks(orig_text: str) -> List[str]:\n",
    "    prompt = f\"\"\"Task: Extract Knowledge parts from question to use in a RAG system\n",
    "        \n",
    "        Please extract sub questions from the following question. Each sub-question should ask for distinct, self-contained units of information. Return the subquestions as a JSON array, ensuring that each item is a question. Use the format: {{\"sub_questions\": [\"question1\", \"question2\"]}}\n",
    "        \n",
    "        Text:\n",
    "        {orig_text}\n",
    "        \"\"\"\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are an assistant that takes apart a question into sub-questions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    answer_ = json.loads(completion.choices[0].message.content)\n",
    "\n",
    "    parts_ = []\n",
    "    if \"sub_questions\" not in answer_:\n",
    "        print(f\"Error in answer: {answer_}\")\n",
    "        return parts_\n",
    "\n",
    "    for know_part in answer_[\"sub_questions\"]:\n",
    "        parts_.append(know_part)\n",
    "\n",
    "    return parts_"
   ],
   "id": "8ba7b7bf341e94f3",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T14:04:29.145478Z",
     "start_time": "2024-07-06T14:04:26.396321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "knowledge_chunks = fetch_knowledge_chunks(chunks[1])\n",
    "for kc in knowledge_chunks:\n",
    "    print(f\"Chunk: {kc.get_id()}, Num chunks: {kc.total_chunks} \\n {kc.chunk_text}\")\n",
    "    # print(f\"Chunk: {kc.get_id()}, Num chunks: {kc.total_chunks} \\n {kc.chunk_text} \\n Original: {kc.properties['original_text']}\")\n",
    "    print(\"----\")"
   ],
   "id": "a74d87f04bc8b723",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: input-doc_1_0, Num chunks: 5 \n",
      " Hands-on experience: You'll get your hands dirty with vector stores and Large Language Models.\n",
      "----\n",
      "Chunk: input-doc_1_1, Num chunks: 5 \n",
      " Combining vector stores and LLMs: We help you combine these two in a way you've never done before.\n",
      "----\n",
      "Chunk: input-doc_1_2, Num chunks: 5 \n",
      " Traditional keyword-based searches: You've probably used search engines for keyword-based searches.\n",
      "----\n",
      "Chunk: input-doc_1_3, Num chunks: 5 \n",
      " Introduction to semantic search: Semantic search is the next big thing after traditional searches.\n",
      "----\n",
      "Chunk: input-doc_1_4, Num chunks: 5 \n",
      " Example of semantic search: It’s like moving from asking Google to search 'best pizza places' to 'Where can I find a pizza place that my gluten-intolerant, vegan friend would love?'\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T13:46:45.000350Z",
     "start_time": "2024-07-06T13:46:23.925561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rag4p.integrations.openai import EMBEDDING_SMALL\n",
    "\n",
    "# Create an in memory content store to hold some chunks\n",
    "openai_embedder = OpenAIEmbedder(api_key=key_loader.get_openai_api_key(), embedding_model=EMBEDDING_SMALL)\n",
    "content_store = InternalContentStore(embedder=openai_embedder, metadata=None)\n",
    "\n",
    "for chunk in chunks:\n",
    "    knowledge_chunks = fetch_knowledge_chunks(chunk)\n",
    "    content_store.store(knowledge_chunks)"
   ],
   "id": "ab8f17c770adc274",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing chunk input-doc_0_0: Building a question-answering system: The process of creating a system similar to Siri, Alexa, or Google Assistant that can answer questions.\n",
      "Storing chunk input-doc_0_1: Workshop offering: A hands-on workshop that guides participants through creating a question-answering system.\n",
      "Storing chunk input-doc_0_2: Preferred language for workshop: Python is the preferred language for the workshop.\n",
      "Storing chunk input-doc_0_3: Workshop GUI: The workshop includes a GUI that works with Python to test your application.\n",
      "Storing chunk input-doc_0_4: Using other languages in the workshop: Participants can use languages other than Python, but will not have access to the GUI for testing their application.\n",
      "Storing chunk input-doc_1_0: Hands-on experience: You'll get your hands dirty with vector stores and Large Language Models, combining these two in a novel way.\n",
      "Storing chunk input-doc_1_1: Past experience with search engines: You have probably used search engines for keyword-based searches.\n",
      "Storing chunk input-doc_1_2: Introduction to semantic search: Semantic search is the next big thing after traditional keyword-based searches.\n",
      "Storing chunk input-doc_1_3: Comparison between traditional and semantic search: Traditional search involves keyword queries like 'best pizza places,' while semantic search involves more detailed queries like 'Where can I find a pizza place that my gluten-intolerant, vegan friend would love?'\n",
      "Storing chunk input-doc_2_0: Building an entire pipeline: The process begins with collecting data from various sources and converting it into vectors, which involves mathematical operations, and storing the vectors to answer different queries.\n",
      "Storing chunk input-doc_2_1: Collecting data from various sources: The initial step in building the pipeline involves gathering data from multiple sources.\n",
      "Storing chunk input-doc_2_2: Converting data into vectors: The collected data is transformed into vectors, which involves mathematical processes.\n",
      "Storing chunk input-doc_2_3: Storing vectors for query responses: The vectors are stored in a manner that allows them to be used for answering various queries.\n",
      "Storing chunk input-doc_2_4: Intended outcome: The intended result of building this pipeline is to create a system that functions similar to a mini Google.\n",
      "Storing chunk input-doc_3_0: Repository: A repository is ready to help you set up everything you need on your laptop.\n",
      "Storing chunk input-doc_3_1: Workshop outcome: By the end of the workshop, you will have your question-answering system ready and running.\n",
      "Storing chunk input-doc_3_2: Call to action: Grab your laptop, bring your coding hat, and start building something fantastic together.\n",
      "Storing chunk input-doc_3_3: Workshop experience: The workshop is expected to be enjoyable and exciting.\n",
      "Storing chunk input-doc_4_0: Vector Store: Tools for storing vectors include OpenSearch, Elasticsearch, and Weaviate.\n",
      "Storing chunk input-doc_4_1: Large Language Model: Large Language Models that can be used include OpenAI, HuggingFace, Cohere, PaLM, and Bedrock.\n",
      "Storing chunk input-doc_4_2: Content Extraction Tool: Tools for content extraction include Unstructured and Llama.\n",
      "Storing chunk input-doc_4_3: Pipeline Creation: Tools for creating a pipeline include Langchain and Custom solutions.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T07:25:13.182981Z",
     "start_time": "2024-07-07T07:25:11.769702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"What will we learn?\"\n",
    "result = content_store.find_relevant_chunks(question, max_results=1)\n",
    "\n",
    "found_chunk = result[0]\n",
    "context = found_chunk.properties[\"original_text\"]\n",
    "openai_answer_generator = OpenaiAnswerGenerator(openai_api_key=key_loader.get_openai_api_key())\n",
    "answer = openai_answer_generator.generate_answer(question, context)\n",
    "\n",
    "print(\"Matched sentence:\")\n",
    "print(\n",
    "    f\"Score: {found_chunk.score:.3f}, Chunk: {found_chunk.get_id()}, Num chunks: {found_chunk.total_chunks} \\n{found_chunk.chunk_text}\")\n",
    "print(f\"Context: \\n{context}\")\n",
    "print(f\"\\nAnswer: \\n{answer}\")"
   ],
   "id": "1b200740af8b308b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding relevant chunks for query: What is semantic search and what vector stores are we using?\n",
      "Matched sentence:\n",
      "Score: 0.855, Chunk: input-doc_4_0, Num chunks: 4 \n",
      "Vector Store: Tools for storing vectors include OpenSearch, Elasticsearch, and Weaviate.\n",
      "Context: \n",
      "Some of the highlights of the workshop: \n",
      "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
      "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM, Bedrock)\n",
      "- Use a tool for content extraction (Unstructured, Llama)\n",
      "- Create your pipeline (Langchain, Custom)\n",
      "\n",
      "\n",
      "Answer: \n",
      "I cannot answer what semantic search is using the context provided. However, the vector stores mentioned in the context are OpenSearch, Elasticsearch, and Weaviate.\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T07:39:07.262081Z",
     "start_time": "2024-07-07T07:39:04.220654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"What is semantic search and what vector stores are we using?\"\n",
    "query_parts = fetch_knowledge_question_chunks(question)\n",
    "context_parts = []\n",
    "for part in query_parts:\n",
    "    print(part)\n",
    "    result = content_store.find_relevant_chunks(part, max_results=1)\n",
    "    found_chunk = result[0]\n",
    "    context_parts.append(found_chunk.properties[\"original_text\"])\n",
    "    print(\n",
    "        f\"Score: {found_chunk.score:.3f}, Chunk: {found_chunk.get_id()}, Num chunks: {found_chunk.total_chunks} \\n{found_chunk.chunk_text}\")\n",
    "\n",
    "context = \" \".join(context_parts)\n",
    "openai_answer_generator = OpenaiAnswerGenerator(openai_api_key=key_loader.get_openai_api_key())\n",
    "answer = openai_answer_generator.generate_answer(question, context)\n",
    "\n",
    "print(f\"Context: \\n{context}\")\n",
    "print(f\"\\nAnswer: \\n{answer}\")"
   ],
   "id": "37d08077ba2dc048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is semantic search?\n",
      "Finding relevant chunks for query: What is semantic search?\n",
      "Score: 0.678, Chunk: input-doc_1_2, Num chunks: 4 \n",
      "Introduction to semantic search: Semantic search is the next big thing after traditional keyword-based searches.\n",
      "What vector stores are we using?\n",
      "Finding relevant chunks for query: What vector stores are we using?\n",
      "Score: 0.881, Chunk: input-doc_4_0, Num chunks: 4 \n",
      "Vector Store: Tools for storing vectors include OpenSearch, Elasticsearch, and Weaviate.\n",
      "Context: \n",
      "You'll get your hands dirty with vector stores and Large Language Models, we help you combine these two in a way you've never done before. You've probably used search engines for keyword-based searches, right? Well, prepare to have your mind blown. We'll dive into something called semantic search, which is the next big thing after traditional searches. It’s like moving from asking Google to search \"best pizza places\" to \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\" – you get the idea, right? Some of the highlights of the workshop: \n",
      "- Use a vector store (OpenSearch, Elasticsearch, Weaviate)\n",
      "- Use a Large Language Model (OpenAI, HuggingFace, Cohere, PaLM, Bedrock)\n",
      "- Use a tool for content extraction (Unstructured, Llama)\n",
      "- Create your pipeline (Langchain, Custom)\n",
      "\n",
      "\n",
      "Answer: \n",
      "Semantic search is a type of search that goes beyond traditional keyword-based searches and understands the context and intent of the query. For example, instead of just searching for \"best pizza places,\" semantic search can understand and find \"Where can I find a pizza place that my gluten-intolerant, vegan friend would love?\"\n",
      "\n",
      "The vector stores being used in the workshop are OpenSearch, Elasticsearch, and Weaviate.\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
